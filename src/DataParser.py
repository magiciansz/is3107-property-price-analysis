from bs4 import BeautifulSoup
import pandas as pd
pd.options.mode.chained_assignment = None # Cancel false positive warnings
import os
import numpy as np
import json
import re
import math


class DataParser:
    def __init__(self):
        pass

    ### Amenities Data Transformation Functions
    def parse_kml(self, filename) -> pd.DataFrame:
        """For hawkercenters, supermarkets, kindergartens, gyms, park facilities, Retailpharmacylocations
        These have <Point> child of <Placemark> element.
        For MRTLine, <LineString> child with start and end
        For parking lot, coordinates of edges

        Args:
            filename (string): kml file name

        Returns:
            pd.DataFrame: kml data parsed into dataframe with longitude latitude, usually altitude contains all 0
        """
        with open(filename, 'r') as f:
            s = BeautifulSoup(f, 'xml')
            placemarks = s.find_all(str("Placemark"))

            data_dict = {}
            for placemark in placemarks:
                # extract placemark name or id
                placemark_id = placemark.find('name').text if placemark.find('name') else placemark['id']
                data_dict[placemark_id] = {}
                # extract table dataa
                simple_data = placemark.find_all('SimpleData')
                for sd in simple_data:    
                    data_dict[placemark_id].update({sd['name']: sd.text})                
                if placemark.Point:
                    coor = placemark.find('Point').coordinates.text.split(',', 2)
                    # https://developers.google.com/kml/documentation/kml_tut
                    longitude, latitude, altitude = map(float, coor)                    
                    data_dict[placemark_id].update({'Longitude': longitude, 'Latitude': latitude, 'Altitude': altitude})
                
                elif placemark.LineString:
                    # start, end = placemark.LineString.coordinates.text.split(' ')
                    # some has more than 2 for a line
                    coor = placemark.find('LineString').coordinates.text
                    data_dict[placemark_id].update({'coordinates': coor})

                elif placemark.Polygon:
                    coor = placemark.find('Polygon').outerBoundaryIs.LinearRing.coordinates.text
                    data_dict[placemark_id].update({'coordinates': coor})
                
                else:
                    print('check kml schema')

            df = pd.DataFrame(data_dict).T.reset_index().rename(columns={'index': filename.split(".")[0]})

            # testing
            # print(df.head())
            # print(df['coordinates'].apply(len).nunique())

        print("Retrieve success: {filename}".format(filename = filename))
        return df


    def parse_carpark(self, filename):
        with open(filename, 'r') as f:
            s = BeautifulSoup(f, 'xml')
            placemarks = s.find_all(str("Placemark"))

            data_dict = {}
            for placemark in placemarks:
                placemark_id = placemark.find('name').text if placemark.find('name') else placemark['id']
                data_dict[placemark_id] = {}
                simple_data = placemark.find_all('SimpleData')
                for sd in simple_data:    
                    data_dict[placemark_id].update({sd['name']: sd.text})
                
                # Pentagon example is generated by drawing simple inner and outer shells and then extruding them down to the ground
                coor = placemark.find('Polygon').outerBoundaryIs.LinearRing.coordinates.text
                # TODO insert martin's code on finding middle pt
                    
            df = pd.DataFrame(data_dict).T.reset_index().rename(columns={'index': 'Parking Lot'})
            # testing
            print(df.head())

        return df
        
    def _extract_data(self):
        hawkers = self.parse_kml('HawkerCentresKML.kml')
        pharmacies = self.parse_kml('RetailpharmacylocationsKML.kml')
        # mrt_stations = kml.parse_kml('MasterPlan2003MRTLine.kml')
        gyms = self.parse_kml('GymsSGKML.kml')
        kindergartens = self.parse_kml('Kindergartens.kml')
        carparks = self.parse_kml('URAParkingLotKML.kml')
        parks = self.parse_kml('ParkFacilitiesKML.kml')
        mrt_stations = pd.read_csv('mrt_lrt_data.csv')
    
        combined_dict = {
            'Hawker': hawkers,
            'Pharmacy': pharmacies,
            'Gym': gyms,
            'Kindergarten': kindergartens,
            'Mrt Station': mrt_stations,
            'Carpark': carparks,
            'Park': parks
        }
        return combined_dict
    
    def _rename_lat_long_cols(self, data_dict):
        data_dict['Mrt Station'] = data_dict['Mrt Station'].rename(columns={'lat':'Latitude', 'lng': 'Longitude'})
        for key, df in data_dict.items():
            df.rename(columns={"Latitude": "lat", "Longitude": "long"}, inplace=True)
        return data_dict
    
    def _add_amenity_type(self, data_dict):
        for amenity_name, amenity_df in data_dict.items():
            print("Amenity type added: {amenity_name}".format(amenity_name = amenity_name))
            data_dict[amenity_name]['Amenity_type'] = amenity_name
        return data_dict
    
    def _add_mid_pt(self, amenity_dict):
        def calculate_mid_pt(df, coords_col):
            def get_avg(lst):
                return sum(lst) / len(lst)
                
            coords_srs = df[coords_col]
            avg_lat_list = []
            avg_long_list = []
            avg_alt_list = []
            counter = 0
            for coords_str in coords_srs:
                try:
                    coord_list = re.split(',| ', coords_str)
                    coord_list = [float(value) for value in coord_list]
                    lat_list = coord_list[1::3]
                    long_list = coord_list[0::3]
                    alt_list = coord_list[2::3]
            
                    avg_lat_list.append(get_avg(lat_list))
                    avg_long_list.append(get_avg(long_list))
                    avg_alt_list.append(get_avg(alt_list))
                    counter += 1
                except:
                    avg_lat_list.append(pd.NA)
                    avg_long_list.append(pd.NA)
                    avg_alt_list.append(pd.NA)
            df['Latitude'] = avg_lat_list
            df['Longitude'] = avg_long_list
            df['Altitude'] = avg_alt_list
            df = df.drop(columns=coords_col)
            return df
            
        for amenity_name, amenity_df in amenity_dict.items():
            try:
                amenity_dict[amenity_name] = calculate_mid_pt(amenity_df, 'coordinates')
                print("Midpoint success: {amenity_name}".format(amenity_name = amenity_name))
            except Exception as e:
                continue
        return amenity_dict
    
    def _save_individual_df(self, data_dict, folder_path):
        for name, df in data_dict.items():
            file_path = "{folder_path}/{name}.csv".format(folder_path=folder_path, name=name)
            df.to_csv(file_path, index=False)
        print("Data saved in {file_path}".format(file_path=file_path))
    
    def _combine_dict_to_df(self, combined_dict, common_cols):
        df_list = []
        common_cols = ["Amenity_type", "lat", "long"] # "Latitude", "Longitude"
        for key, df in combined_dict.items():
            df_common_cols = df[common_cols]
            df_list.append(df_common_cols)
            combined_df = pd.concat(df_list, ignore_index=True)
        # combined_df = combined_df.reset_index().rename(columns={'index':'Amenity_id'})
        return combined_df
    
    def amenity_data_transformation_pipeline(self, out_folder_path):
        amenity_dict = self._extract_data()
        amenity_dict = self._add_amenity_type(amenity_dict)
        amenity_dict = self._add_mid_pt(amenity_dict)
        amenity_dict = self._rename_lat_long_cols(amenity_dict)
    
        # Save individual transformed amenities
        self._save_individual_df(amenity_dict, out_folder_path)
    
        # Combine all amenities into one dataframe and save
        common_cols = ["Amenity_type", "lat", "long"]
        combined_df = self._combine_dict_to_df(amenity_dict, common_cols)
        file_path = "{folder_path}/Combined_amenities.csv".format(folder_path=out_folder_path)
        combined_df.to_csv(file_path)
        return combined_df
    
    ### HDB Data Transformation Functions
    def parse_hdb(self, file_path) -> pd.DataFrame:
        """parse hdb resale data

        Args:
            file_path: hdb dataset file path

        Returns:
            dataframe: cleaned data with specified columns in db
        """
        hdb_df = pd.read_csv(file_path).drop("Unnamed: 0", axis=1, errors='ignore')

        hdb_df.insert(1, 'lease_duration', 99)
        hdb_df.insert(1, 'type_of_sale', 'HDB Resale')
        try:
            hdb_df.insert(0, "transaction_year", None)
        except:
            print("year column already added")
        year_month_col = hdb_df["month"].str.split("-")
        hdb_df["transaction_year"] = year_month_col.str[0].astype(np.int64)
        hdb_df["month"] = year_month_col.str[1].astype(np.int64)
        # hdb_df.rename(columns={'month':'transaction_month'}, inplace=True)

        storey_cols = hdb_df["storey_range"].str.split(" ")
        hdb_df["floor_range_start"] = storey_cols.str[0].astype(np.int64)
        hdb_df["floor_range_end"] = storey_cols.str[2].astype(np.int64)
        hdb_df = hdb_df.drop('storey_range', axis=1)
        hdb_df['remaining_lease'] = hdb_df['remaining_lease'].apply(lambda x: self._convert_format(x))

        # standardise column names
        hdb_df.columns = ['transaction_year', 'transaction_month', 'type_of_sale', 'lease_duration', 'town_hdb', 'property_type', 'block', 'street', 'floor_area', 'flat_model', 'lease_start_year', 'remaining_lease', 'resale_price', 'address', 'lat', 'long', 'planning_area', 'floor_range_start', 'floor_range_end'] 
        
        # define property_id from duplicated property-specific info
        hdb_df['property_id'] = hdb_df.groupby([
                                        # 'transaction_year',
                                        # 'transaction_month',
                                        'type_of_sale',
                                        'lease_duration',
                                        'town_hdb',
                                        'property_type',
                                        'block',
                                        'street',
                                        'floor_area',
                                        'flat_model',
                                        'lease_start_year',
                                        # 'remaining_lease',
                                        # 'resale_price',
                                        'address',
                                        'lat',
                                        'long',
                                        'planning_area',
                                        'floor_range_start',
                                        'floor_range_end'], 
                                        sort=False).ngroup()
        
        return hdb_df

    def _convert_format(self, remaining_lease):
        """helper function to convert lease format

        Args:
            remaining_lease : xx years xx months

        Returns:
            string: xxYxxM
        """
        rl = remaining_lease.split()
        years = rl[0] + 'Y'
        if len(rl) > 2:
            months = rl[2] + 'M'
        else:
            months = '00M'
        return years + months


    ### URA Data Transformation Functions        
    # Flatten "Transaction" column from URA dataset
    def _unnest(self, df, col) -> pd.DataFrame:
        """flattern `transaction`, define project_id and property_id

        Args:
            df (DataFrame): combined URA data batches
            col (_type_): `Transaction`

        Returns:
            DataFrame: check project_id and property_id definition
        """
        col_flat = pd.DataFrame([[i, x] 
                           for i, y in df[col].items() 
                               for x in y], columns=['Property_index', col])
        col_flat = col_flat.set_index('Property_index')[col]
        col_flat_df = pd.DataFrame(list(col_flat), index = col_flat.index)
        df = df.drop(columns=[col])
        df = df.merge(col_flat_df, left_index=True, right_index=True)
        df['project_id'] = df.groupby(['project']).ngroup()
        
        df['property_id'] = df.groupby(['street',
                                        'x',
                                        #  'project',
                                        'y',
                                        #  'marketSegment',
                                        'lat',
                                        'long',
                                        'planning_area',
                                        'area',
                                        'floorRange',
                                        'noOfUnits',
                                        #  'contractDate',
                                        #  'typeOfSale',
                                        #  'price',
                                        'propertyType',
                                        'district',
                                        'typeOfArea',
                                        'tenure',
                                        #  'nettPrice',
                                        #  'project_id'
                                        ]
                                        ).ngroup()
        return df

    # Extract lease_year and lease_duration from "tenure" column in URA dataset
    def _extract_lease_year_and_duration(self, df, target_col):
        df_result = df.copy()

        # Extract lease_duration
        pattern = r'(\d+) yrs'
        # df_result['lease_duration'] = df_result[target_col].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else 9999).astype(int)
        df_result['lease_duration'] = df_result[target_col].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else "")
        df_result['lease_duration'] = df_result['lease_duration'].apply(lambda x: int(x) if x.isdigit() else pd.NA)
        
        # Extract lease_year
        pattern = r'from (\d+)'
        # df_result['lease_year'] = df_result[target_col].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else -1).astype(int)
        df_result['lease_year'] = df_result[target_col].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else "")
        df_result['lease_year'] = df_result['lease_year'].apply(lambda x: int(x) if x.isdigit() else pd.NA)

        # TODO for discussion set lease_year and lease_duration for 'Freehold' tenure
        # df_result[df_result['lease_duration'].isna()]['tenure'].unique()

        return df_result

    # Extract floor_range_start and floor_range_end from "floorRange" column in URA dataset
    def _extract_floor_range(self, df, target_col):
        def convert_basement_to_negative(floor_range_start, floor_range_end):
            floor_range_start = floor_range_start.str.replace('B', '-')
            floor_range_end = floor_range_end.str.replace('B', '-')
            return floor_range_start, floor_range_end
        df_result = df.copy()
        floor_range = df_result[target_col].str.split('-')
        try:
            floor_range_start = floor_range.str[0]
            floor_range_end = floor_range.str[1]
            floor_range_start, floor_range_end = convert_basement_to_negative(floor_range_start, floor_range_end)
        except:
            floor_range_start = pd.NA
            floor_range_end = pd.NA
        df_result['floor_range_start'] = floor_range_start
        df_result['floor_range_end'] = floor_range_end
        
        # TODO for discussion: "floorRange": "B1-B5" -> NAN
        df_result['floor_range_start'] = pd.to_numeric(df_result['floor_range_start'], errors='coerce').astype('Int64')
        df_result['floor_range_end'] = pd.to_numeric(df_result['floor_range_end'], errors='coerce').astype('Int64')

        return df_result

    # Extract transaction_month and transaction_year function from "contractDate" column in URA dataset
    def _extract_transaction_month_and_year(self, df, target_col):
        df_result = df.copy()
        df_result['transaction_month'] = df_result[target_col].str.slice(0, 2).astype(int)
        df_result['transaction_year'] = 2000 + df_result[target_col].str.slice(-2).astype(int)
        return df_result

    # Replace values for typeOfSale column in URA dataset
    def _convert_type_of_sale(self, df, target_col):
        df_result = df.copy()
        df_result = df_result.replace({target_col: {'1': 'New sale', '2': 'Sub sale', '3': 'Resale'}})
        return df_result

    # Save URA dataset from dataframe to json format
    def _save_ura_dataset(self, df, file_path):
        df_dict = df.to_dict('records')
        json_data = json.dumps({'Result': df_dict})
        with open(file_path, 'w') as file:
            file.write(json_data)
        print("Save success: {file_path}".format(file_path=file_path))

    # URA Data transformation pipeline
    def URA_data_transformation_pipeline(self, folder, file_name_list, file_type):
        """URA data transformation steps

        Args:
            folder (string path): folder to read data from 
            file_name_list (list): data files in input folder
            file_type (filetype): URA json files

        Returns:
            DataFrame: URA dataframe based on the db definition
        """
        private_properties_df_final = []
        for file_name in file_name_list:
            file_path = "{folder}/{file_name}.{file_type}".format(folder=folder, file_name=file_name, file_type=file_type)
            with open(file_path, 'r') as file:
                # Load JSON data from the file
                try:
                    data = json.load(file)['Result']
                except KeyError:
                    data = json.load(data)
                private_properties_df_final.extend(data)
        private_properties_df_final = pd.DataFrame(private_properties_df_final)
        
        # private_properties_df_final = remove_properties_without_latlong(file_name, private_properties_df, 'lat', 'long')
        private_properties_df_final = self._unnest(private_properties_df_final, "transaction")
        private_properties_df_final = private_properties_df_final.drop(columns=["nettPrice"])
        private_properties_df_final = self._extract_lease_year_and_duration(private_properties_df_final, 'tenure')
        private_properties_df_final = self._extract_floor_range(private_properties_df_final, 'floorRange')
        private_properties_df_final = self._extract_transaction_month_and_year(private_properties_df_final, 'contractDate')
        private_properties_df_final = self._convert_type_of_sale(private_properties_df_final, 'typeOfSale')
        
        # naming and type of data according to db definition
        common_cols_dict = {
            # URA_col_name: common_col_name
            "area": "floor_area",
            "district": "district_id",
            "typeOfSale": "type_of_sale",
            "typeOfArea": "property_type",
            "project": "project_name",    
        }
        private_properties_df_final = private_properties_df_final.rename(columns = common_cols_dict)

        dtype_dict = {'x': 'float', 
              'y': 'float', 
              'lat': 'float', 
              'long': 'float',
              'floor_area': 'float', 
              'noOfUnits': 'int', 
              'price': 'float', 
              'district_id': 'int', 
              'lease_year': 'Int32', 
              'lease_duration': 'Int32'
              }
        # Change data types for each column
        private_properties_df_final = private_properties_df_final.astype(dtype_dict)
    
        new_file_name = 'URA_dataset_combined_new'
        new_file_path = "{folder}/{new_file_name}.{file_type}".format(folder=folder, new_file_name=new_file_name, file_type=file_type)
        # self._save_ura_dataset(private_properties_df_final, new_file_path)
        private_properties_df_final.to_csv(new_file_path, index=False)

        return private_properties_df_final

if __name__ == "__main__":
    kml = DataParser()
    # Execute Amenity data transformation pipeline
    
    # amenity_out_folder_path = './Amenity Data [Final]'
    amenity_out_folder_path = './Data'
    amenity_combined_df = kml.amenity_data_transformation_pipeline(amenity_out_folder_path)

    # Execute URA data transformation pipeline

    # URA_folder = './URA Data [Final]'
    URA_folder = './Data'

    URA_file_name_list = ['privatepropertypricesbatch1added', 
                          'privatepropertypricesbatch2added',
                      'privatepropertypricesbatch3added', 'privatepropertypricesbatch4added'
                          ]
    URA_file_type = 'json'
    URA_combined_df = kml.URA_data_transformation_pipeline(URA_folder, URA_file_name_list, URA_file_type)
    print(URA_combined_df.head())
    print(URA_combined_df.info())

# Execute HDB data transformation pipeline
# hdb = kml.parse_hdb('./Data/hdb_resale_full.csv')
# TODO: Add HDB.property_id and project_id, number is taken from last number of URA_combined.property_id and project_id
# HDB_property_id_start = URA_combined_df['property_id'].iloc[-1] + 1
# HDB_project_id_start = URA_combined_df['project_id'].iloc[-1] + 1
# print(hdb.head())
# print(hdb.info())

# TODO: Combine HDB and URA dataset via combined columns (also add property_id and project_id to HDB, starting number is after URA's proeprty_id and project_id)
#*URA Dataset need to be first (eg: index 0-100), then HDB Dataset (eg: index 101-200), because URA.property_id comes from 'unflatten' function

# TODO: Split the Combined Dataset into Property Table and Transaction Table with their relevant columns